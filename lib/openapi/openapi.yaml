openapi: 3.0.1
info:
  title: worldiety mistral
  version: 1.20.0
  description: |
    *mistral* is a very specialized database powered by *worldiety* which supports the storage and retrieval of time 
    based sensor metrics for devices. 
    It scales quite well for billions of values on a single node.
    
    ## FAQ
    
    ### What is a timestamp?
    Mistral expects that all time series keys represent a 64bit signed UTC timestamp in seconds since Epoch 1970.
    Intentionally, neither a single timestamp nor an entire time series carries information about time zones or UTC offsets.
    Time zones become only relevant for certain use cases and are not required for general purpose.
    Mistral expects that a time zone is a question of presentation and is either defined by the location
    of a physical data source (e.g. a wind generator) or a legal (and physical) accounting location.
    
    ### Why is there no ISO 8601 or RFC 3339 support?
    Both standards cannot express time zones.
    What they represent is a relation between a local time and an offset to the UTC.
    Therefore, one cannot use these standards to perform certain data aggregations, because _random_ offsets like 
    daylight saving times cannot be calculated from that offset.
    Even worse, they are not intuitive and humans get it often wrong.
    
    > Example: What is the _difference_ between `2019-10-12T07:20:50.52Z` and `2019-10-12T14:20:50.52+07:00`?
    
    ### What is a time zone?
    We refer to the proceedings of the RFC 6557 which defines, how the IANA time zones are maintained.
    A time zone is defined as a `area/location` tuple, which refers to a unique physical region in history or in present.
    This is a defacto standard and has been accepted by all major operating systems.
    It contains rules about historic and current offsets between the UTC and the locations offset or daylight saving times.
    Currently, there are nearly 600 regions (including links) with individual rules defined.
    Strictly speaking, the special area _Etc_ is artificial and does not refer to a distinct location and we do not consider
    that a real time zone. 
    Hence, usage of _Etc_ is discouraged and implementations are allowed to reject processing.
    
    > Example: `America/New_York` or `Europe/Berlin`.
    
    ### How is an aggregation handled on daylight saving time?
    The time series points do not carry any time zone information and are just the Unix time stamps (UTC seconds).
    However, when aggregating a range and a time zone must be specified.
    
    > Example: `(2038-01-19 03:14:07,2038-01-19 03:14:07]@Europe/Berlin`. Here, _from_ is exclusive and _to_ is inclusive, time zone is Berlin.
    
    When using the build-in aggregate functions, you can define an additional _drift_ value to respect start- or end-aggregated
    values before applying the according time zone offsets. Using this _drift_ one can shift the value in a way, so that
    they fall in or out of a certain year, month or day interval.
    As a consequence, depending on the time zone, a day may contain 23 hours or 25 hours when switching between daylight saving 
    times, which is intentional.
    
    ### Why is there no native float support?
    Floating point values suggest a high accuracy, but mostly they are not. 
    Due to the nature of sensors, the sampled measurement values are discrete and _exact_ (in a way).
    
    > Example: A Sensirion SHTC3 temperature sensor uses only 2 bytes to quantize its measurements. Also this is even
      well beyond its actual accuracy and already contains a lot of noise. 
    
    What is worse, floats introduce a lot of noise to numbers alone which otherwise must be calculated in a lossless
    way. 
    When aggregating sums over millions or even billions of values, these tiny errors add up and makes explanations hard,
    especially if a sum represents a production amount which has a 1:1 relation to money - and nobody wants floats
    in their banking accounts.
    
    > Example: `0.1` is actually `0.100000001490116119384765625` in IEEE-754.
    
    Also, this noise makes efficient compression a tough task.
    Therefore, mistral does not support storing floats and expects pre-scaled values so that values can be expressed
    using integers. 
    However, using a compute kernel, one can post-scale those integers for display purposes.
    
    ### Why is there no CSV import or export?
    CSV is not suited as a reliable data exchange format.
    End users often create documents with different order and naming of columns.
    Also, things like the encoding (e.g. unicode BOM) or just the serialization of floating point values are not specified.
    It is not in the scope of mistral, to support and maintain these kinds of compatibility problems.

    ### How do I store time stamp related meta data like OPC quality?
    Due to indexing and speed concerns, a time series point is always a tuple of two distinct 64 bit signed integer
    values (16 byte in total). Therefore, additional meta data cannot be attached directly.
    However, to store meta data like OPC quality, best practice in mistral is to create an additional time series, which
    contains the associated quality codes.
    
    > Example: An OPC DA Quality Code can be expressed using a 2 byte integer. Things like different _Bad_, _Uncertain_
    or _Good_ measurements can be expressed.

    Tip: Always store related time series within a single bucket to ensure atomic data consistency.
    
    ### How do I store latitude/longitude data for each time series point?
    Similar to _OPC quality_, such data can only be stored within a different time series.
    Additionally, latitude and longitude values are naturally double values.
    Just as discussed in _Why is there no native float support?_, you can simply scale the coordinates, e.g. using
    a scale of 10^7 which already allows a single digit centimeter resolution. 
    
    > Tip: Do not represent floats as IEEE 754 binary integer representation, to avoid processing signal noise. 
    
    Keep in mind, that most GPS location data e.g. from smartphones have an effective resolution of multiple meters.
    
    ### How do I store multiple values per time stamp?
    Technically there are no reasons, why the storage engine could not handle multiple time stamps,
    however we decided to not support them. When applying the WAL, the values of existing time stamps
    are updated in the appended order. 
    A time series is a strict functional mapping, which means having unique index values.
    Allowing otherwise, would result in a lot of questions with unclear answers: Which order
    do these multiple timestamps have? When aggregating, which and how are multiple values handled? 
    Is the semantic of all values the same? When creating a relation to other time series, which timestamp
    should be the foreign key?
    
    > Tip: If you have a time series and want to fix time stamps and have a later comparison between those
    values (e.g. for machine learning), create a unique time series for each evolution cycle. 
    Then, you can easily process them using the group or translate functions.
    
    ### How can I correct data?
    As long as the timestamp is correct, you can simply write the new points into the according bucket endpoint.
    If the timestamps are not correct, you have to remove points by using a range expression.
    Afterwards, you can insert new points as usual.
    Even though this is handled through the WAL, one cannot express atomicity in combination.
    However, your middleware can narrow this by flushing the bucket first, submitting the changes into the WAL
    within the specified interval and flush (or just wait) afterwards.
    
    > Note: The primary goal of Mistral is performance, also traded in by eventual consistency.
    A read-after write consistency or an explicit transaction mechanism are non-goals.
    
    ### Which kind of database is used?
    Time series data is indexed, compressed and stored in a custom format.
    The write ahead log (WAL) is kept in memory until flushed either by a request or due to a time interval (default 1 hour).
    When flushing, the log data is merged into the already persisted data set and optimized for reading, which
    characterizes the implementation as a log-structured merge tree. 
    Merging happens atomically (ACID) per bucket and is safe as far as a correct _fsync_ implementation is provided.
    On a crash, the in-memory WAL is lost and must be recovered by the middleware.
    
    > Warning: Use enterprise grade SSD drives and a filesystem with proper fsync support. Mistral is optimized for
    batch-insertion and read-heavy workloads and trades performance versus consistency using the WAL. The middleware must
    be properly tuned to trade available server resources, required eventual consistency windows and query performance.
    
    Meta data is stored differently, using a separate transactional key-value store.

    ### How is the time series data organized?
    A bucket defines a namespace in which multiple time series can be stored.
    Changes are captured within an in-memory write ahead log (WAL).
    Technically, a bucket is always consistent and ACID semantics are guaranteed when applying or merging the WAL.
    As a consequence, ACID semantics are not applied for appending to the WAL itself, which is why the mistral
    database should be considered as eventual consistent, especially there is no general read-after-write consistency. 
    Currently, the default commit interval is 1 hour (configurable) and the WAL is not persistent.
    To mitigate data loss, a middleware can either provide a resumeable workflow or enforce merging the WAL by using the _x-flush_ flag.
    
    > Tip: As best practice, represent each data source (like a wind generator) as a bucket. 
    Create a UUID within your middleware to address your device and use this ID to identify the according bucket.
    
    A time series is basically a sorted list of 64-bit integer XY-points where the _x_-value represents the unique primary key in
    UTC seconds.
    A time series itself is also identified using a UUID, which must be unique within a bucket.
    Conventionally, different time series sharing the same semantics across different buckets, should always
    be addressed by the same ID.
    
    > Tip: As best practice, all time series using the same semantics like _production in kWh_ should be
    addressed by the same ID across all buckets (e.g. different wind generators).
    
    Also conventionally, a Descriptor can be saved within the meta data storage of mistral, which
    is used to define certain aspects of a time series.
    Usually this descriptor is created first, before any actual time series data is written and its semantics
    must not change - most importantly the scale must be constant over the entire lifetime.
    Because of this write-once-read-many characteristics, these meta data are stored within a different 
    storage implementation and are not coupled with any bucket (or actual time series).
    
    > Tip: Define and create a Descriptor for a time series before actually inserting the first time series point
    into any bucket.
    
    ### Commit window and possibilities of data loss
    To optimize performance, the WAL is kept in memory. On the other hand this increases the possibilities of 
    data loss for unwritten WAL entries in case of a server outage. 
    The default commit interval of 1 hour means to loose at most the last six 10min values per bucket.
    Therefore, it is recommend that your middleware has a resume functionality to replay the last few critical write
    operations. 
    In practice, this is not a problem, because typical data sources already provide a history of the last aggregated
    time series samples anyway and if not, you can still fallback to the (inefficient) _x-flush_ flag to enforce
    immediate consistency per bucket.
    
    > Tip: When running within a docker container, ensure that the shutdown timeout interval matches your WAL size
    and machine performance. Consider a larger timeout like 5 minutes or larger, to mitigate data loss on regular 
    shutdowns. Mistral is highly concurrent and immediately profits from faster SSDs.
    
    Mistral guarantees (as long as the operating system, the file system and the hardware are sane), that
    a WAL merge never corrupts a bucket. 
    Hence, Mistral is always consistent, either you end up with the state before applying the WAL or with
    the state after applying the WAL. 

    > Note: we are evaluating a persistent WAL which lowers the risk of data loss by trading IOPS and throughput.
    
    By default, Mistral calculates and checks the consistency of the entire dataset by validating all data points
    using a cryptographic hash sum (sha256, FIPS 180-4) at startup time. This may take quite a few minutes but can be disabled, however
    it verifies that there was no silent data corruption.
    
    ### What platforms are supported?
    We recommend a current Linux and a reliable file system you trust, e.g. a current 
    Ubuntu LTS distribution and the latest Ext filesystem (tier 1). 
    MacOS is only supported for evaluation and development purposes and must not be used in production (tier 2). 
    Currently, Windows and 32bit platforms are unsupported.
    
    > Tip: Feel free to [contact us](https://www.worldiety.de/produkte/mistral) and we will try to assess your 
    needs and may introduce new supported platforms.
    
    
    ### Why newline delimited json (NDJSON)?
    Intentionally, the json format is not generally suited to be written or read partially - especially arrays, which
    needs grammatically balanced brackets.
    There is no common standard and there are a lot of variations in the industry to solve this in different ways.
    Probably one of the most common solutions which is well known in the area of structured logging, is to write a minified
    json object per line. 
    To formalize this, we refer to the [NDJSON specification](https://github.com/ndjson/ndjson-spec) and follow this approach
    for raw time series data tuples.
    This format should not be used by compute kernels, because these are intended to reduce and prepare data for visualization,
    which usually requires to return mostly a few hundred tuples and a normal JSON object is just fine for that.
    

    ### What hardware to use?
    This cannot be answered in a general way and depends on the dataset, the kind of queries and the amount parallel users.
    There are use cases where you can aggregate billions of values on the cheapest cloud server and there are also
    use cases where even the largest machine is not enough.
    
    > Tip: Feel free to [contact us](https://www.worldiety.de/produkte/mistral) and we will try to assess your needs and help you with appropriate benchmarks if necessary.
    
    For evaluation, starting with a 16 core processor, 64GB Ram and a 512GB nvme-SSD should be fine.

  license:
    name: worldiety Enterprise Edition (EE) Lizenz (die "EE Lizenz")
    url: https://worldiety.github.io/mistral/LICENSE


  x-logo:
    url: 'https://worldiety.github.io/mistral/wdy-logo.svg'
    altText: 'worldiety logo'
    href: https://www.worldiety.de/produkte/mistral


servers:
  - url: "https://mistral.worldiety.net"



tags:
  - name: status
    description: |
      This resource provides basic health information about the service.
      It is conventionally provided to support a hosting within complex infrastructures 
      like kubernetes. A successful response does not mean that everything is actually
      fine. It just executes some basic checks and inspections to prove that in general
      the service should be able to process requests.

  - name: buckets
    description: |
      Resources of _buckets_ provide access to both, meta data and actual time series data.
      A bucket represents usually a device like a wind generator but may also be used
      for other time-based data required e.g. for accounting.
      
      Advanced querying and aggregation is only possible by using the computational kernel API.
  - name: bucketgroups
    description: |
      Resources of _bucketgroups_ represent groups of buckets (or devices), often known as portfolios.
      This resource is purely virtual and can be used to simplify the development and 
      usage of computational kernels. 
      
      For example, it may make sense to create a group
      representing a clients portfolio of wind generators he owns. Then, a compute kernel
      may just take the group identifier to load all contained bucket identifiers itself.

  - name: kernels
    description: |
      Resources of _kernels_ represent the compute kernels written in the MiEl language - 
      a subset respective dialect of the Go programming language. 
      You should avoid the creation or modification of kernels from end-users, to
      protect your service against DoS-like attacks. Even though the execution is sandboxed
      and that there is no standard library available, a kernel can still consume an unreasonable
      amount of CPU or memory resources.
      
      Mistral distributes and schedules these kernels as execution is requested.
      A kernel usually assembles a pipeline of pre-compiled fixed-function aggregation
      algorithms and transforms the result into an arbitrary (json) structure.
      This avoid to transfer and serialize large sets of data through the network, which
      otherwise would degrade system performance multiple orders of magnitude.
      Thus, always keep in mind to bring the computation to the data (your MiEL compute kernel)
      instead of just serializing the data over the wire into your middleware.
      
      Mistral also provides a bunch of build-in and ready-to-use kernels, which cannot be modified and
      may be optimized and updated in future releases. These kernels are especially useful
      for basic inspection and data analysis tasks.
      
      Example for a MiEl compute kernel:
      
      ```go
      package main

      import (
        "context"
        miel "github.com/worldiety/mistral/lib/go/dsl/v1"
      )

      type Request struct {
        Buckets []miel.UUID `json:"buckets"`
        Metric  miel.UUID   `json:"metric"`
        Range   miel.Range  `json:"range"`
      }

      type Response struct {
        BucketNames []string    `json:"names"`
        MetricName  string      `json:"metric"`
        Data        miel.FGroup `json:"data"`
      }

      func Declare() (interface{}, interface{}) {
        return Request{
            Buckets: []miel.UUID{miel.NewUUID()},
            Metric:  miel.NewUUID(),
            Range:   "[2038-01-19 03:14:07,2038-01-19 03:14:07]@Europe/Berlin",
          }, Response{
            BucketNames: []string{"wind generator"},
            MetricName:  "Windspeed in km/h",
            Data:        miel.FGroup{miel.FPoints{miel.FPoint{X: 1648826693, Y: 3.14}}},
          }
      }

      func Eval(ctx context.Context) {
        var request Request
        miel.Request(ctx, &request)

        scale := miel.Query(ctx).ScaleOf(request.Metric)
        width := miel.ViewportWidth(ctx)

        miel.Query(ctx).
          FindInRange(request.Buckets, request.Metric, request.Range.MustInterval()).
          ForEachF(func(pts miel.Points) miel.FPoints {
            return pts.Downscale(width).Unscale(scale)
          })

        miel.Response(ctx, Response{})
      }

      func main() {
        miel.Configure().
          Parameter(Declare).
          Start(Eval)
      }
      ```

  - name: timeseries
    description: |
      Resources of _timeseries_ allows access to ranges of stored time series points within a specific
      bucket. 
      All time series data within a bucket and time series is treated uniquely per time stamp, which should
      be represented as a Unix timestamp in seconds since Epoch, which is also expected by
      all [MiEl kernel functions](#tag/kernels).

  - name: descriptors
    description: |
      Resources of _descriptors_ contain bucket-wide meta data identified by the ID of a time series.
      
      Time series data is shared per bucket and uniquely addressed using a time series id.
      To commonly manage information about a specific time series, these resource can be used.
      Technically, the time series descriptor meta data is separated from the actual time series data.
      Even though, they share the same identifiers, they have nothing in common and are only
      conventionally related. This also affects the transaction semantics: changing meta data
      are not propagated through the time series WAL. 
      Therefore, the existence of meta data for metrics is not related to the existence of actual time series
      data within a metric shard of a bucket.
      The following rules of thumb are helpful:
      * create the descriptor before importing any time series data, to ensure that a compute kernel has access to the defined meta data, especially like the scale.
      * do never change the semantics of a time series, especially sampling or scaling must not be changed. Instead, create a new time series, otherwise computations will likely be wrong. 

paths:

  #########################################################
  # Status endpoint
  #########################################################

  /healthz:
    $ref: './v1/status/paths.yaml#/healthz'

  /api/v1/descriptors:
    $ref: './v1/descriptor/paths.yaml#/descriptors'

  /api/v1/descriptors/{id}:
    $ref: './v1/descriptor/paths.yaml#/descriptors-{id}'

  /api/v1/buckets:
    $ref: './v1/buckets/paths.yaml#/buckets'

  /api/v1/buckets/{id}:
    $ref: './v1/buckets/paths.yaml#/buckets-{id}'

  /api/v1/buckets/{bucket-id}/timeseries/{ts-id}:
    $ref: './v1/buckets/timeseries.yaml#/timeseries'

  /api/v1/bucketgroups:
    $ref: './v1/bucketgroups/paths.yaml#/bucketgroups'

  /api/v1/bucketgroups/{id}:
    $ref: './v1/bucketgroups/paths.yaml#/bucketgroups-{id}'

  /api/v1/kernels:
    $ref: './v1/kernels/paths.yaml#/kernels'


  /api/v1/kernels/{id}:
    $ref: './v1/kernels/paths.yaml#/kernels-{id}'

  /api/v1/eval/kernel:
    $ref: './v1/kernels/eval.yaml#/eval'

  /api/v1/kernels/{id}/run:
    $ref: './v1/kernels/run.yaml#/run'

  /api/v1/kernels/{id}/parameter:
    $ref: './v1/kernels/params.yaml#/params'

  /api/v1/merges/timeseries:
    $ref: './v1/bulk/paths.yaml#/merges'

  /api/v1/renames/buckets:
    $ref: './v1/bulk/paths.yaml#/renames'

components:
  securitySchemes:
    bearerAuth:
      type: http
      scheme: bearer
      description: |
        Securitywise, mistral does not provide its own authentication or authorization
        infrastructure. The intended modus operandi locates mistral into a private
        network, with a custom gateway API or middleware in front of it. 
        There is no ownership of buckets, time series data or users in general, which
        also means, that this kind of authorization must be guaranteed by the according 
        upstream service. Therefore, there is only a single basic level of security, based
        on a secret bearer token, which must be exchanged through an external secure channel.
        Usually, you want to exchange and deploy these secrets at least through
        dynamic environment variables within your deployment infrastructure.