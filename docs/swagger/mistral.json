{
  "openapi": "3.0.1",
  "info": {
    "title": "worldiety mistral",
    "version": "1.20.0",
    "description": "*mistral* is a very specialized database powered by *worldiety* which supports the storage and retrieval of time \nbased sensor metrics for devices. \nIt scales quite well for billions of values on a single node.\n\n## FAQ\n\n### What is a timestamp?\nMistral expects that all time series keys represent a 64bit signed UTC timestamp in seconds since Epoch 1970.\nIntentionally, neither a single timestamp nor an entire time series carries information about time zones or UTC offsets.\nTime zones become only relevant for certain use cases and are not required for general purpose.\nMistral expects that a time zone is a question of presentation and is either defined by the location\nof a physical data source (e.g. a wind generator) or a legal (and physical) accounting location.\n\n### Why is there no ISO 8601 or RFC 3339 support?\nBoth standards cannot express time zones.\nWhat they represent is a relation between a local time and an offset to the UTC.\nTherefore, one cannot use these standards to perform certain data aggregations, because _random_ offsets like \ndaylight saving times cannot be calculated from that offset.\nEven worse, they are not intuitive and humans get it often wrong.\n\n> Example: What is the _difference_ between `2019-10-12T07:20:50.52Z` and `2019-10-12T14:20:50.52+07:00`?\n\n### What is a time zone?\nWe refer to the proceedings of the RFC 6557 which defines, how the IANA time zones are maintained.\nA time zone is defined as a `area/location` tuple, which refers to a unique physical region in history or in present.\nThis is a defacto standard and has been accepted by all major operating systems.\nIt contains rules about historic and current offsets between the UTC and the locations offset or daylight saving times.\nCurrently, there are nearly 600 regions (including links) with individual rules defined.\nStrictly speaking, the special area _Etc_ is artificial and does not refer to a distinct location and we do not consider\nthat a real time zone. \nHence, usage of _Etc_ is discouraged and implementations are allowed to reject processing.\n\n> Example: `America/New_York` or `Europe/Berlin`.\n\n### How is an aggregation handled on daylight saving time?\nThe time series points do not carry any time zone information and are just the Unix time stamps (UTC seconds).\nHowever, when aggregating a range and a time zone must be specified.\n\n> Example: `(2038-01-19 03:14:07,2038-01-19 03:14:07]@Europe/Berlin`. Here, _from_ is exclusive and _to_ is inclusive, time zone is Berlin.\n\nWhen using the build-in aggregate functions, you can define an additional _drift_ value to respect start- or end-aggregated\nvalues before applying the according time zone offsets. Using this _drift_ one can shift the value in a way, so that\nthey fall in or out of a certain year, month or day interval.\nAs a consequence, depending on the time zone, a day may contain 23 hours or 25 hours when switching between daylight saving \ntimes, which is intentional.\n\n### Why is there no native float support?\nFloating point values suggest a high accuracy, but mostly they are not. \nDue to the nature of sensors, the sampled measurement values are discrete and _exact_ (in a way).\n\n> Example: A Sensirion SHTC3 temperature sensor uses only 2 bytes to quantize its measurements. Also this is even\n  well beyond its actual accuracy and already contains a lot of noise. \n\nWhat is worse, floats introduce a lot of noise to numbers alone which otherwise must be calculated in a lossless\nway. \nWhen aggregating sums over millions or even billions of values, these tiny errors add up and makes explanations hard,\nespecially if a sum represents a production amount which has a 1:1 relation to money - and nobody wants floats\nin their banking accounts.\n\n> Example: `0.1` is actually `0.100000001490116119384765625` in IEEE-754.\n\nAlso, this noise makes efficient compression a tough task.\nTherefore, mistral does not support storing floats and expects pre-scaled values so that values can be expressed\nusing integers. \nHowever, using a compute kernel, one can post-scale those integers for display purposes.\n\n### Why is there no CSV import or export?\nCSV is not suited as a reliable data exchange format.\nEnd users often create documents with different order and naming of columns.\nAlso, things like the encoding (e.g. unicode BOM) or just the serialization of floating point values are not specified.\nIt is not in the scope of mistral, to support and maintain these kinds of compatibility problems.\n\n### How do I store time stamp related meta data like OPC quality?\nDue to indexing and speed concerns, a time series point is always a tuple of two distinct 64 bit signed integer\nvalues (16 byte in total). Therefore, additional meta data cannot be attached directly.\nHowever, to store meta data like OPC quality, best practice in mistral is to create an additional time series, which\ncontains the associated quality codes.\n\n> Example: An OPC DA Quality Code can be expressed using a 2 byte integer. Things like different _Bad_, _Uncertain_\nor _Good_ measurements can be expressed.\n\nTip: Always store related time series within a single bucket to ensure atomic data consistency.\n\n### How do I store latitude/longitude data for each time series point?\nSimilar to _OPC quality_, such data can only be stored within a different time series.\nAdditionally, latitude and longitude values are naturally double values.\nJust as discussed in _Why is there no native float support?_, you can simply scale the coordinates, e.g. using\na scale of 10^7 which already allows a single digit centimeter resolution. \n\n> Tip: Do not represent floats as IEEE 754 binary integer representation, to avoid processing signal noise. \n\nKeep in mind, that most GPS location data e.g. from smartphones have an effective resolution of multiple meters.\n\n### How do I store multiple values per time stamp?\nTechnically there are no reasons, why the storage engine could not handle multiple time stamps,\nhowever we decided to not support them. When applying the WAL, the values of existing time stamps\nare updated in the appended order. \nA time series is a strict functional mapping, which means having unique index values.\nAllowing otherwise, would result in a lot of questions with unclear answers: Which order\ndo these multiple timestamps have? When aggregating, which and how are multiple values handled? \nIs the semantic of all values the same? When creating a relation to other time series, which timestamp\nshould be the foreign key?\n\n> Tip: If you have a time series and want to fix time stamps and have a later comparison between those\nvalues (e.g. for machine learning), create a unique time series for each evolution cycle. \nThen, you can easily process them using the group or translate functions.\n\n### How can I correct data?\nAs long as the timestamp is correct, you can simply write the new points into the according bucket endpoint.\nIf the timestamps are not correct, you have to remove points by using a range expression.\nAfterwards, you can insert new points as usual.\nEven though this is handled through the WAL, one cannot express atomicity in combination.\nHowever, your middleware can narrow this by flushing the bucket first, submitting the changes into the WAL\nwithin the specified interval and flush (or just wait) afterwards.\n\n> Note: The primary goal of Mistral is performance, also traded in by eventual consistency.\nA read-after write consistency or an explicit transaction mechanism are non-goals.\n\n### Which kind of database is used?\nTime series data is indexed, compressed and stored in a custom format.\nThe write ahead log (WAL) is kept in memory until flushed either by a request or due to a time interval (default 1 hour).\nWhen flushing, the log data is merged into the already persisted data set and optimized for reading, which\ncharacterizes the implementation as a log-structured merge tree. \nMerging happens atomically (ACID) per bucket and is safe as far as a correct _fsync_ implementation is provided.\nOn a crash, the in-memory WAL is lost and must be recovered by the middleware.\n\n> Warning: Use enterprise grade SSD drives and a filesystem with proper fsync support. Mistral is optimized for\nbatch-insertion and read-heavy workloads and trades performance versus consistency using the WAL. The middleware must\nbe properly tuned to trade available server resources, required eventual consistency windows and query performance.\n\nMeta data is stored differently, using a separate transactional key-value store.\n\n### How is the time series data organized?\nA bucket defines a namespace in which multiple time series can be stored.\nChanges are captured within an in-memory write ahead log (WAL).\nTechnically, a bucket is always consistent and ACID semantics are guaranteed when applying or merging the WAL.\nAs a consequence, ACID semantics are not applied for appending to the WAL itself, which is why the mistral\ndatabase should be considered as eventual consistent, especially there is no general read-after-write consistency. \nCurrently, the default commit interval is 1 hour (configurable) and the WAL is not persistent.\nTo mitigate data loss, a middleware can either provide a resumeable workflow or enforce merging the WAL by using the _x-flush_ flag.\n\n> Tip: As best practice, represent each data source (like a wind generator) as a bucket. \nCreate a UUID within your middleware to address your device and use this ID to identify the according bucket.\n\nA time series is basically a sorted list of 64-bit integer XY-points where the _x_-value represents the unique primary key in\nUTC seconds.\nA time series itself is also identified using a UUID, which must be unique within a bucket.\nConventionally, different time series sharing the same semantics across different buckets, should always\nbe addressed by the same ID.\n\n> Tip: As best practice, all time series using the same semantics like _production in kWh_ should be\naddressed by the same ID across all buckets (e.g. different wind generators).\n\nAlso conventionally, a Descriptor can be saved within the meta data storage of mistral, which\nis used to define certain aspects of a time series.\nUsually this descriptor is created first, before any actual time series data is written and its semantics\nmust not change - most importantly the scale must be constant over the entire lifetime.\nBecause of this write-once-read-many characteristics, these meta data are stored within a different \nstorage implementation and are not coupled with any bucket (or actual time series).\n\n> Tip: Define and create a Descriptor for a time series before actually inserting the first time series point\ninto any bucket.\n\n### Commit window and possibilities of data loss\nTo optimize performance, the WAL is kept in memory. On the other hand this increases the possibilities of \ndata loss for unwritten WAL entries in case of a server outage. \nThe default commit interval of 1 hour means to loose at most the last six 10min values per bucket.\nTherefore, it is recommend that your middleware has a resume functionality to replay the last few critical write\noperations. \nIn practice, this is not a problem, because typical data sources already provide a history of the last aggregated\ntime series samples anyway and if not, you can still fallback to the (inefficient) _x-flush_ flag to enforce\nimmediate consistency per bucket.\n\n> Tip: When running within a docker container, ensure that the shutdown timeout interval matches your WAL size\nand machine performance. Consider a larger timeout like 5 minutes or larger, to mitigate data loss on regular \nshutdowns. Mistral is highly concurrent and immediately profits from faster SSDs.\n\nMistral guarantees (as long as the operating system, the file system and the hardware are sane), that\na WAL merge never corrupts a bucket. \nHence, Mistral is always consistent, either you end up with the state before applying the WAL or with\nthe state after applying the WAL. \n\n> Note: we are evaluating a persistent WAL which lowers the risk of data loss by trading IOPS and throughput.\n\nBy default, Mistral calculates and checks the consistency of the entire dataset by validating all data points\nusing a cryptographic hash sum (sha256, FIPS 180-4) at startup time. This may take quite a few minutes but can be disabled, however\nit verifies that there was no silent data corruption.\n\n### What platforms are supported?\nWe recommend a current Linux and a reliable file system you trust, e.g. a current \nUbuntu LTS distribution and the latest Ext filesystem (tier 1). \nMacOS is only supported for evaluation and development purposes and must not be used in production (tier 2). \nCurrently, Windows and 32bit platforms are unsupported.\n\n> Tip: Feel free to [contact us](https://www.worldiety.de/produkte/mistral) and we will try to assess your \nneeds and may introduce new supported platforms.\n\n\n### Why newline delimited json (NDJSON)?\nIntentionally, the json format is not generally suited to be written or read partially - especially arrays, which\nneeds grammatically balanced brackets.\nThere is no common standard and there are a lot of variations in the industry to solve this in different ways.\nProbably one of the most common solutions which is well known in the area of structured logging, is to write a minified\njson object per line. \nTo formalize this, we refer to the [NDJSON specification](https://github.com/ndjson/ndjson-spec) and follow this approach\nfor raw time series data tuples.\nThis format should not be used by compute kernels, because these are intended to reduce and prepare data for visualization,\nwhich usually requires to return mostly a few hundred tuples and a normal JSON object is just fine for that.\n\n\n### What hardware to use?\nThis cannot be answered in a general way and depends on the dataset, the kind of queries and the amount parallel users.\nThere are use cases where you can aggregate billions of values on the cheapest cloud server and there are also\nuse cases where even the largest machine is not enough.\n\n> Tip: Feel free to [contact us](https://www.worldiety.de/produkte/mistral) and we will try to assess your needs and help you with appropriate benchmarks if necessary.\n\nFor evaluation, starting with a 16 core processor, 64GB Ram and a 512GB nvme-SSD should be fine.\n",
    "license": {
      "name": "worldiety Enterprise Edition (EE) Lizenz (die \"EE Lizenz\")",
      "url": "https://worldiety.github.io/mistral/LICENSE"
    },
    "x-logo": {
      "url": "https://worldiety.github.io/mistral/wdy-logo.svg",
      "altText": "worldiety logo",
      "href": "https://www.worldiety.de/produkte/mistral"
    }
  },
  "servers": [
    {
      "url": "https://mistral.worldiety.net"
    }
  ],
  "tags": [
    {
      "name": "status",
      "description": "This resource provides basic health information about the service.\nIt is conventionally provided to support a hosting within complex infrastructures \nlike kubernetes. A successful response does not mean that everything is actually\nfine. It just executes some basic checks and inspections to prove that in general\nthe service should be able to process requests.\n"
    },
    {
      "name": "buckets",
      "description": "Resources of _buckets_ provide access to both, meta data and actual time series data.\nA bucket represents usually a device like a wind generator but may also be used\nfor other time-based data required e.g. for accounting.\n\nAdvanced querying and aggregation is only possible by using the computational kernel API.\n"
    },
    {
      "name": "bucketgroups",
      "description": "Resources of _bucketgroups_ represent groups of buckets (or devices), often known as portfolios.\nThis resource is purely virtual and can be used to simplify the development and \nusage of computational kernels. \n\nFor example, it may make sense to create a group\nrepresenting a clients portfolio of wind generators he owns. Then, a compute kernel\nmay just take the group identifier to load all contained bucket identifiers itself.\n"
    },
    {
      "name": "kernels",
      "description": "Resources of _kernels_ represent the compute kernels written in the MiEl language - \na subset respective dialect of the Go programming language. \nYou should avoid the creation or modification of kernels from end-users, to\nprotect your service against DoS-like attacks. Even though the execution is sandboxed\nand that there is no standard library available, a kernel can still consume an unreasonable\namount of CPU or memory resources.\n\nMistral distributes and schedules these kernels as execution is requested.\nA kernel usually assembles a pipeline of pre-compiled fixed-function aggregation\nalgorithms and transforms the result into an arbitrary (json) structure.\nThis avoid to transfer and serialize large sets of data through the network, which\notherwise would degrade system performance multiple orders of magnitude.\nThus, always keep in mind to bring the computation to the data (your MiEL compute kernel)\ninstead of just serializing the data over the wire into your middleware.\n\nMistral also provides a bunch of build-in and ready-to-use kernels, which cannot be modified and\nmay be optimized and updated in future releases. These kernels are especially useful\nfor basic inspection and data analysis tasks.\n\nExample for a MiEl compute kernel:\n\n```go\npackage main\n\nimport (\n  \"context\"\n  miel \"github.com/worldiety/mistral/lib/go/dsl/v1\"\n)\n\ntype Request struct {\n  Buckets []miel.UUID `json:\"buckets\"`\n  Metric  miel.UUID   `json:\"metric\"`\n  Range   miel.Range  `json:\"range\"`\n}\n\ntype Response struct {\n  BucketNames []string    `json:\"names\"`\n  MetricName  string      `json:\"metric\"`\n  Data        miel.FGroup `json:\"data\"`\n}\n\nfunc Declare() (interface{}, interface{}) {\n  return Request{\n      Buckets: []miel.UUID{miel.NewUUID()},\n      Metric:  miel.NewUUID(),\n      Range:   \"[2038-01-19 03:14:07,2038-01-19 03:14:07]@Europe/Berlin\",\n    }, Response{\n      BucketNames: []string{\"wind generator\"},\n      MetricName:  \"Windspeed in km/h\",\n      Data:        miel.FGroup{miel.FPoints{miel.FPoint{X: 1648826693, Y: 3.14}}},\n    }\n}\n\nfunc Eval(ctx context.Context) {\n  var request Request\n  miel.Request(ctx, &request)\n\n  scale := miel.Query(ctx).ScaleOf(request.Metric)\n  width := miel.ViewportWidth(ctx)\n\n  miel.Query(ctx).\n    FindInRange(request.Buckets, request.Metric, request.Range.MustInterval()).\n    ForEachF(func(pts miel.Points) miel.FPoints {\n      return pts.Downscale(width).Unscale(scale)\n    })\n\n  miel.Response(ctx, Response{})\n}\n\nfunc main() {\n  miel.Configure().\n    Parameter(Declare).\n    Start(Eval)\n}\n```\n"
    },
    {
      "name": "timeseries",
      "description": "Resources of _timeseries_ allows access to ranges of stored time series points within a specific\nbucket. \nAll time series data within a bucket and time series is treated uniquely per time stamp, which should\nbe represented as a Unix timestamp in seconds since Epoch, which is also expected by\nall [MiEl kernel functions](#tag/kernels).\n"
    },
    {
      "name": "descriptors",
      "description": "Resources of _descriptors_ contain bucket-wide meta data identified by the ID of a time series.\n\nTime series data is shared per bucket and uniquely addressed using a time series id.\nTo commonly manage information about a specific time series, these resource can be used.\nTechnically, the time series descriptor meta data is separated from the actual time series data.\nEven though, they share the same identifiers, they have nothing in common and are only\nconventionally related. This also affects the transaction semantics: changing meta data\nare not propagated through the time series WAL. \nTherefore, the existence of meta data for metrics is not related to the existence of actual time series\ndata within a metric shard of a bucket.\nThe following rules of thumb are helpful:\n* create the descriptor before importing any time series data, to ensure that a compute kernel has access to the defined meta data, especially like the scale.\n* do never change the semantics of a time series, especially sampling or scaling must not be changed. Instead, create a new time series, otherwise computations will likely be wrong. \n"
    }
  ],
  "paths": {
    "/health": {
      "get": {
        "operationId": "GetStatus",
        "tags": [
          "status"
        ],
        "summary": "Shows some health metrics about this service.",
        "description": "A Status follows more or less the https://tools.ietf.org/id/draft-inadarei-api-health-check-01.html draft.\n",
        "responses": {
          "200": {
            "description": "successful operation",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/Status"
                }
              }
            }
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/descriptors": {
      "get": {
        "operationId": "ListDescriptors",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "descriptors"
        ],
        "summary": "Returns a list of all descriptors.",
        "description": "A descriptor may have additional meta data, like a name or information about the origin from which it has been imported.\n",
        "responses": {
          "200": {
            "description": "A Descriptor defines the nature of a time series. \nA _time series_ is defined as a series of key-value tuples, where the key is a unique Unix time stamp and the value is an integer.\nAll tuples are ordered ascending based on the time stamp.\nThe nature of a Descriptor must be constant for all associated time series tuples over all time.\nTo programmatically decide, which kind of calculations are valid, a Descriptor contains various meta information to describe what and how the value must be interpreted.\nA Descriptor must be quantifiable and must have a unit.\nTypically, a Descriptor is a _physical quantity_ or a business Descriptor like a _key performance indicator_.\n",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/Descriptor"
                  }
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/descriptors/{id}": {
      "get": {
        "operationId": "GetDescriptor",
        "summary": "Returns the descriptor for a single time series.",
        "description": "A time series always has an attached set of meta data, like a name or information about the origin from which it has been imported.",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "descriptors"
        ],
        "parameters": [
          {
            "name": "id",
            "description": "The time series identifier.",
            "in": "path",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "successful operation",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/Descriptor"
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "delete": {
        "operationId": "DeleteDescriptor",
        "summary": "Removes the meta data and all time series in all buckets.",
        "description": "<img src=\"https://worldiety.github.io/mistral/exclamation.svg\" width=\"32\">\n\nThis endpoint resides in the dangerzone because it is not entirely protected\nby transactions and very I/O intensive. \nThis endpoint was created to support refactorings of your middleware and \nshould otherwise never be used.\n\nIn all buckets including the descriptor meta data and time series data is removed. \nThe current implementation is always blocking and returns a 204 on success. \nIf the service crashes or fails with a 500, better start consulting your backup.\n",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "descriptors"
        ],
        "parameters": [
          {
            "name": "id",
            "description": "The descriptor identifier.",
            "in": "path",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "put": {
        "operationId": "SaveDescriptor",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "descriptors"
        ],
        "summary": "Create or update a time series descriptor.",
        "description": "A time series may have additional meta data, like a name or information about the origin from which it has been imported.\nBuckets are not modified.\n",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "The time series identifier.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/Descriptor"
              }
            }
          }
        },
        "responses": {
          "201": {
            "$ref": "#/components/responses/C201"
          },
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/buckets": {
      "get": {
        "operationId": "ListBuckets",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "buckets"
        ],
        "summary": "Returns the meta data for all available buckets.",
        "description": "This endpoint returns the entire set of all available devices (or time series buckets in general). This just contains the meta data.",
        "responses": {
          "200": {
            "description": "successful operation",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/Bucket"
                  }
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/buckets/{id}": {
      "get": {
        "operationId": "GetBucket",
        "summary": "Returns the meta data for a single bucket or device.",
        "description": "A bucket always has an attached set of meta data, like a name or information about the origin from which it has been imported.",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "buckets"
        ],
        "parameters": [
          {
            "name": "id",
            "description": "The bucket identifier.",
            "in": "path",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "successful operation",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/Bucket"
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "delete": {
        "operationId": "DeleteBucket",
        "summary": "Remove the entire bucket.",
        "description": "The entire bucket including meta data and time series data is removed. \nThe current implementation is always blocking and returns a 204 on success. \n",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "buckets"
        ],
        "parameters": [
          {
            "name": "id",
            "description": "The bucket identifier.",
            "in": "path",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "put": {
        "operationId": "SaveBucket",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "buckets"
        ],
        "summary": "Creates or updates the meta data for the bucket.",
        "description": "A device may have additional meta data, like a name or information about the origin from which it has been imported.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "The bucket identifier.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/Bucket"
              }
            }
          }
        },
        "responses": {
          "201": {
            "$ref": "#/components/responses/C201"
          },
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/buckets/{bucket-id}/timeseries/{ts-id}": {
      "get": {
        "operationId": "GetPoints",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "timeseries"
        ],
        "summary": "returns time series data.",
        "description": "Returns an entire range of unfiltered time series data as a stream.\n",
        "parameters": [
          {
            "name": "bucket-id",
            "in": "path",
            "description": "Identifier of the bucket (e.g. a device).",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          },
          {
            "name": "ts-id",
            "in": "path",
            "description": "Identifier of the time series within the bucket.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          },
          {
            "name": "interval",
            "in": "query",
            "description": "Range is a string representation of a range. ( or ] can be used to indicate exclusive and inclusive intervals.\n( or ) means exclusive and [ or ] means inclusive.\n\nFormat specification:\n\n  ```< [|( > < min >, < max > < ]|) > @ < IANA time zone name >```\n",
            "required": false,
            "schema": {
              "$ref": "#/components/schemas/Range"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "A PointStream is like a JSON array of points, but actually it is not JSON.\nInstead it uses a newline delimited json object encoding, as described by https://github.com/ndjson/ndjson-spec.\nServer and client implementations are encouraged to use a chunked encoding to avoid full buffering, so\nexpect that your implementation should parse and process until _EOF_ (end of file).\n\n```json\n{\"x\": 1653988963, \"y\": 42}\\n\n{\"x\": 1653988964, \"y\": 43}\\n\n{\"x\": 1653988965, \"y\": 44}\\n\n```\n\n_Tip: Avoid consuming point streams and instead use the [kernels API](#tag/kernels) for processing, which are\nmultiple orders of magnitudes faster. Remember, that even a laptop processor can reach a memory bandwidth of 800GB/s but a 10GBit\nLAN can only provide 1,2GB/s plus cycles and bandwidth for serialization and deserialization._\n",
            "content": {
              "application/x-ndjson": {
                "schema": {
                  "$ref": "#/components/schemas/PointStream"
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "delete": {
        "operationId": "DeletePoints",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "timeseries"
        ],
        "summary": "delete time series data.",
        "description": "This endpoint provides the possibility to remove an entire range of time series data for a specific bucket (like a device) and metric.\n",
        "parameters": [
          {
            "name": "X-Flush",
            "in": "header",
            "description": "This header is optional. Default is false to just append it to the _write ahead log_ (WAL). \nDepending on the servers configuration, this may take the entire commit window (default is 1 hour).\nIf true, a database flush is enforced which may block and make changes immediately visible. \nIt will compact and shard the data and write it atomically into the servers filesystem. \nAfterwards, the device index is reloaded and the data can be read back.\nDepending on the load, this may cause massive performance penalties and may even cause timeouts, especially\nwhen used concurrently or if the machine is already under load or the _write ahead log_ is huge.\n\nIf true, the response code is 204 on success, otherwise a 202.\n",
            "required": false,
            "schema": {
              "type": "boolean",
              "default": false
            }
          },
          {
            "name": "bucket-id",
            "in": "path",
            "description": "Identifier of the bucket (e.g. a device).",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          },
          {
            "name": "ts-id",
            "in": "path",
            "description": "Identifier of the time series within the bucket.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          },
          {
            "name": "interval",
            "in": "query",
            "description": "Range is a string representation of a range. ( or ] can be used to indicate exclusive and inclusive intervals.\n( or ) means exclusive and [ or ] means inclusive.\n\nFormat specification:\n\n  ```< [|( > < min >, < max > < ]|) > @ < IANA time zone name >```\n",
            "required": false,
            "schema": {
              "$ref": "#/components/schemas/Range"
            }
          }
        ],
        "responses": {
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "post": {
        "operationId": "PutPoints",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "timeseries"
        ],
        "summary": "Insert, append or update metric data.",
        "description": "A post will create a device, if required. Given time series data is either appended, inserted or already existing\nvalues are updated. The data is only available after a flush, which may usually only happen once an hour. However, for\ntesting purposes or if you are going to calculate carefully and insert huge amounts of data (gigabytes), you can flush\nthe data explicitly.\n\n**Warning**: you should never put a single value for all of your devices and call flush for each.\nThis will hurt your servers performance seriously! Rule of thumb: if you are not sure, do not use the _X-Flush_ parameter.\n\nThe X axis of the dataset is stored as a strict monotonic arbitrary integer.\nHowever, group functions (like group by day) interpret the value\nas a Unix timestamp in seconds.\n",
        "parameters": [
          {
            "name": "X-Flush",
            "in": "header",
            "description": "This header is optional. Default is false to just append it to the _write ahead log_ (WAL). \nDepending on the servers configuration, this may take the entire commit window (default is 1 hour).\nIf true, a database flush is enforced which may block and make changes immediately visible. \nIt will compact and shard the data and write it atomically into the servers filesystem. \nAfterwards, the device index is reloaded and the data can be read back.\nDepending on the load, this may cause massive performance penalties and may even cause timeouts, especially\nwhen used concurrently or if the machine is already under load or the _write ahead log_ is huge.\n\nIf true, the response code is 204 on success, otherwise a 202.\n",
            "required": false,
            "schema": {
              "type": "boolean",
              "default": false
            }
          },
          {
            "name": "bucket-id",
            "in": "path",
            "description": "Identifier of the bucket (e.g. a device).",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          },
          {
            "name": "ts-id",
            "in": "path",
            "description": "Identifier of the time series within the bucket.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "description": "A PointStream is like a JSON array of points, but actually it is not JSON.\nInstead it uses a newline delimited json object encoding, as described by https://github.com/ndjson/ndjson-spec.\nServer and client implementations are encouraged to use a chunked encoding to avoid full buffering, so\nexpect that your implementation should parse and process until _EOF_ (end of file).\n\n```json\n{\"x\": 1653988963, \"y\": 42}\\n\n{\"x\": 1653988964, \"y\": 43}\\n\n{\"x\": 1653988965, \"y\": 44}\\n\n```\n\n_Tip: Avoid consuming point streams and instead use the [kernels API](#tag/kernels) for processing, which are\nmultiple orders of magnitudes faster. Remember, that even a laptop processor can reach a memory bandwidth of 800GB/s but a 10GBit\nLAN can only provide 1,2GB/s plus cycles and bandwidth for serialization and deserialization._\n",
          "content": {
            "application/x-ndjson": {
              "schema": {
                "$ref": "#/components/schemas/PointStream"
              }
            }
          }
        },
        "responses": {
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/bucketgroups": {
      "get": {
        "operationId": "ListBucketGroups",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "bucketgroups"
        ],
        "summary": "Returns the all available bucket groups.",
        "description": "This endpoint returns the entire set of all available groups of buckets.",
        "responses": {
          "200": {
            "description": "Returns the group data.",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/BucketGroup"
                  }
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/bucketgroups/{id}": {
      "get": {
        "operationId": "GetBucketGroup",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "bucketgroups"
        ],
        "summary": "Returns the bucket meta data for the id.",
        "description": "This endpoint returns the meta data about a single stored bucket group.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of the bucket group.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Returns the buckets meta data.",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/BucketGroup"
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "put": {
        "operationId": "SaveBucketGroup",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "bucketgroups"
        ],
        "summary": "Creates or updates the denoted bucket group.",
        "description": "Create or update a bucket group containing reference identifiers to buckets. If the resource does not yet exist, it will be created.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of the bucket group.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/BucketGroup"
              }
            }
          }
        },
        "responses": {
          "201": {
            "$ref": "#/components/responses/C201"
          },
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "delete": {
        "operationId": "DeleteBucketGroup",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "bucketgroups"
        ],
        "summary": "Removes a bucket group.",
        "description": "Removes the specified bucket group. All referenced buckets are kept alive.\nDeleting a non-existing bucket group will also return 202 or 204.\n",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of bucket group.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/kernels": {
      "get": {
        "operationId": "ListKernels",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Returns all compute kernels and their according meta data.",
        "description": "This endpoint returns the entire set of all available stored procedures.",
        "responses": {
          "200": {
            "description": "Returns the list of kernels.",
            "content": {
              "application/json": {
                "schema": {
                  "type": "array",
                  "items": {
                    "$ref": "#/components/schemas/Kernel"
                  }
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/kernels/{id}": {
      "get": {
        "operationId": "LoadKernel",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Returns the meta data for a specific proc.",
        "description": "This endpoint returns the meta data about a single stored procedure.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of the compute kernel stored procedure.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Returns the compute kernel.",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/Kernel"
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "put": {
        "operationId": "SaveKernel",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Updates the kernel.",
        "description": "The kernel consists of a few meta data and the actual MiEl compute kernel script. If the resource does not yet exist, it is created.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of the kernel respective stored procedure.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/Kernel"
              }
            }
          }
        },
        "responses": {
          "201": {
            "$ref": "#/components/responses/C201"
          },
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      },
      "delete": {
        "operationId": "DeleteKernel",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Removes the kernel.",
        "description": "Removes a compute kernel and its meta data. Deleting a non-existing kernel is not an error.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of the stored procedure.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/eval/kernel": {
      "post": {
        "operationId": "EvalKernel",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Evaluates a non-persistent compute kernel.",
        "description": "Useful to avoid that one-shot scripts must be stored, loaded and deleted. Provides instant validation feedback to the user.",
        "parameters": [
          {
            "in": "header",
            "name": "X-TZ",
            "schema": {
              "$ref": "#/components/schemas/Timezone"
            }
          },
          {
            "in": "header",
            "name": "Viewport-Width",
            "schema": {
              "$ref": "#/components/schemas/ViewportWidth"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "params": {
                    "$ref": "#/components/schemas/KernelParam"
                  },
                  "src": {
                    "description": "src contains the MiEl compute kernel which shall be executed using the given header and kernel parameters.",
                    "$ref": "#/components/schemas/MiEl"
                  }
                }
              }
            }
          }
        },
        "responses": {
          "200": {
            "$ref": "#/components/responses/KernelResult"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/kernels/{id}/run": {
      "post": {
        "operationId": "RunKernel",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Executes a stored compute kernel.",
        "description": "Loads the kernel respective stored procedure by its ID and execute it with the given parameters submitted as a json body. See also",
        "parameters": [
          {
            "in": "header",
            "name": "X-TZ",
            "schema": {
              "$ref": "#/components/schemas/Timezone"
            }
          },
          {
            "in": "header",
            "name": "Viewport-Width",
            "schema": {
              "$ref": "#/components/schemas/ViewportWidth"
            }
          },
          {
            "name": "id",
            "in": "path",
            "description": "ID of the compute kernel.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/KernelParam"
              }
            }
          }
        },
        "responses": {
          "200": {
            "$ref": "#/components/responses/KernelResult"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/kernels/{id}/parameter": {
      "get": {
        "operationId": "GetParams",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "kernels"
        ],
        "summary": "Returns the input and output parameter definition, if available.",
        "description": "This endpoint returns the examples and structures about the request input and response output parameter specification. The MiEL code will get executed partially to get this information.",
        "parameters": [
          {
            "name": "id",
            "in": "path",
            "description": "ID of the stored procedure.",
            "required": true,
            "schema": {
              "$ref": "#/components/schemas/UUID"
            }
          }
        ],
        "responses": {
          "200": {
            "description": "Returns the input and output parameter data as declared by the kernel.",
            "content": {
              "application/json": {
                "schema": {
                  "$ref": "#/components/schemas/ParamInfo"
                }
              }
            }
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "404": {
            "$ref": "#/components/responses/C404"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/merges/timeseries": {
      "post": {
        "operationId": "MergeTimeSeries",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "timeseries"
        ],
        "summary": "Merge time series into each other.",
        "description": "Takes the given transitions and applies them eventually from old to new. Old values are deleted and put on top of potentially existing new values. Flush will force an immediate consistency. If used wrong (e.g. single metric merge and flush sequentially), flush will cause a massive write amplification which can break the servers' storage system (e.g. 13 EiB to write for 700 metrics requiring 20TiB disk usage). It is valid to merge empty or not existing metrics into existing. It is also valid to merge existing metrics into empty or not existing metrics. Note that due to the nature of distinct meta and metrics databases, the consistency between the two cannot be guaranteed. So a failure may result in an invalid divergent state, even though each of them may still be consistent from their individual perspective.",
        "parameters": [
          {
            "name": "X-Flush",
            "in": "header",
            "description": "force flush, blocks and makes changes immediately visible. This hurts performance seriously.",
            "required": false,
            "schema": {
              "type": "boolean",
              "default": false
            }
          }
        ],
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/BulkSeriesMergeMapping"
              }
            }
          }
        },
        "responses": {
          "202": {
            "$ref": "#/components/responses/C202"
          },
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    },
    "/api/v1/renames/buckets": {
      "post": {
        "operationId": "RenameBuckets",
        "security": [
          {
            "bearerAuth": []
          }
        ],
        "tags": [
          "buckets"
        ],
        "summary": "Renames bucket identifiers.",
        "description": "<img style=\"display:inline\" src=\"https://worldiety.github.io/mistral/exclamation.svg\" width=\"32\">\n\nThis endpoint resides in the dangerzone because it is not entirely protected\nby transactions and very I/O intensive. \nThis endpoint was created to support data migrations and refactorings of your middleware and \nshould otherwise never be used. Always create a backup before using. If a target identifier already exists,\nthis operation will fail. See also the [time series merges](#tag/timeseries/operation/MergeTimeSeries) operation.\n\nTechnically, a global lock is acquired to protect against weired (logical) races, while performing a\nrename, so inserting more data will likely cause a timeout. A change of an ID will rewrite the entire bucket, so changing all\nIDs will rewrite your entire dataset. While rewriting, the entire server is blocked and will issue timeouts\non all endpoints (besides healthz). If the server crashes while performing this operation, the result may\nbe an inconsistent data set, which may have files named after old ids causing a data loss after writing\nto the new or old id again. So if you encounter a crash, better restore from a clean backup and try again.\nWhile loading, we try to detect such broken data sets and reject to start. Note that all pending changes\nare flushed before. If successful, there is no more pending data left and everything is consistent.\n\nThe metadata is updated similarly, however metadata is treated entirely optional. The metadata of old\nreplaces the metadata of new entirely. If old does not exist, new is replaced with empty defaults. If old\nexists it will be deleted.\n",
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "$ref": "#/components/schemas/BulkBucketRename"
              }
            }
          }
        },
        "responses": {
          "204": {
            "$ref": "#/components/responses/C204"
          },
          "400": {
            "$ref": "#/components/responses/C400"
          },
          "403": {
            "$ref": "#/components/responses/C403"
          },
          "500": {
            "$ref": "#/components/responses/C500"
          }
        }
      }
    }
  },
  "components": {
    "securitySchemes": {
      "bearerAuth": {
        "type": "http",
        "scheme": "bearer",
        "description": "Securitywise, mistral does not provide its own authentication or authorization\ninfrastructure. The intended modus operandi locates mistral into a private\nnetwork, with a custom gateway API or middleware in front of it. \nThere is no ownership of buckets, time series data or users in general, which\nalso means, that this kind of authorization must be guaranteed by the according \nupstream service. Therefore, there is only a single basic level of security, based\non a secret bearer token, which must be exchanged through an external secure channel.\nUsually, you want to exchange and deploy these secrets at least through\ndynamic environment variables within your deployment infrastructure.\n"
      }
    },
    "schemas": {
      "Status": {
        "type": "object",
        "description": "A Status follows more or less the https://tools.ietf.org/id/draft-inadarei-api-health-check-01.html draft.\n",
        "required": [
          "status",
          "version",
          "releaseID",
          "description"
        ],
        "properties": {
          "status": {
            "type": "string",
            "enum": [
              "pass",
              "warn",
              "fail"
            ],
            "description": "e.g. pass or fail or warn.",
            "nullable": false
          },
          "version": {
            "type": "string",
            "description": "the vcs (git) commit hash.",
            "nullable": false
          },
          "releaseID": {
            "type": "string",
            "description": "a semantic version like v1.2.3.",
            "nullable": false
          },
          "description": {
            "type": "string",
            "description": "some details about the service.",
            "nullable": false
          },
          "details": {
            "type": "object",
            "description": "arbitrary map to describe status of components.",
            "nullable": false
          }
        }
      },
      "InvalidParam": {
        "type": "object",
        "description": "InvalidParam contains a name and reason tuple to describe a field related problem, typically used by\nfor form validation.\n",
        "example": {
          "name": ".address.firstname",
          "reason": "firstname must not be empty."
        },
        "required": [
          "name",
          "reason"
        ],
        "properties": {
          "name": {
            "type": "string",
            "format": "jq",
            "description": "The jq or javascript compatible field selector.",
            "nullable": false
          },
          "reason": {
            "type": "string",
            "description": "Reason is the localized message to handout to the end-user.",
            "nullable": false
          }
        }
      },
      "ProblemDetails": {
        "type": "object",
        "description": "A ProblemDetails type describes a problem with field selector extensions using\nrfc7807 - see also https://datatracker.ietf.org/doc/html/rfc7807.\n",
        "example": {
          "type": "ora://validation-error",
          "title": "Invalid value.",
          "status": 400,
          "detail": "Your firstname must not be empty.",
          "instance": "trace://550e8400-e29b-11d4-a716-446655440000",
          "invalid-params": [
            {
              "name": ".address.firstname",
              "reason": "Your firstname must not be empty."
            }
          ]
        },
        "required": [
          "type",
          "title",
          "status",
          "details",
          "instance"
        ],
        "properties": {
          "type": {
            "type": "string",
            "format": "uri",
            "nullable": false,
            "description": "Consumers MUST use the 'type' string as the primary identifier for\nthe problem type. This must not be a URL but may also be a URI.\n"
          },
          "title": {
            "type": "string",
            "default": "",
            "nullable": false,
            "description": "A short, human-readable summary of the problem\ntype.  It SHOULD NOT change from occurrence to occurrence of the\nproblem, except for purposes of localization.\n"
          },
          "status": {
            "type": "integer",
            "nullable": false,
            "format": "int32",
            "description": "The HTTP status code."
          },
          "detail": {
            "type": "string",
            "nullable": false,
            "default": "",
            "description": "A human-readable explanation specific to this\noccurrence of the problem.\n"
          },
          "instance": {
            "type": "string",
            "nullable": false,
            "format": "uri",
            "description": "A URI reference that identifies the specific\noccurrence of the problem. It contains a random\nuuid to identify each problem individually and which\ncan be used to trace within the log files. Intentionally,\nthis cannot be dereferenced to not leak implementation details\nand therefore increase the attack surface.\n"
          },
          "invalid-params": {
            "type": "array",
            "nullable": false,
            "description": "InvalidParam contains a name and reason tuple to describe a field related problem, typically used by\nfor form validation.\n",
            "items": {
              "$ref": "#/components/schemas/InvalidParam"
            }
          }
        }
      },
      "UUID": {
        "type": "string",
        "x-go-type": "string",
        "format": "uuid",
        "description": "The unique ID of the resource in the canonical UUID format.",
        "example": "550e8400-e29b-11d4-a716-446655440000"
      },
      "Period": {
        "type": "string",
        "description": "A Period describes the base interval of a sampling.\nThe following intervals are specified:\n  * `10m`: the time series value consists of whatever has been measured within a constant interval of 600 seconds.\n  * `15m`: the time series value consists of whatever has been measured within a constant interval of 900 seconds.\n  * `daily (deprecated)`: the time series value consists of whatever has been measured within a time zone specific interval.\n     It is deprecated because this indicates that a time zone dependent data aggregation has already happened which\n     makes sane post-processing nearly impossible. Use a compute [kernel](#tag/kernels) and a variable\n     timezone on a constant interval like 10m. E.g. it is not possible anymore, to aggregate into a month of another\n     timezone.\n   * `monthly (deprecated)`: the time series value consists of whatever has been measured within a time zone specific interval.\n     It is deprecated because this indicates that a time zone dependent data aggregation has already happened which\n     makes sane post-processing nearly impossible. Use a compute [kernel](#tag/kernels) and a variable\n     timezone on a constant interval like 10m. E.g. it is not possible anymore, to aggregate into a year of another\n     timezone.\n   * `none`: the time series value has been measured or calculated without any significant duration.\n     This is only and always the case for samplings of `instant` or `levelBegin` or `levelEnd`.\n\nAdditional intervals may be standardized later. \nImplementations must accept non-standardized intervals, which may be passed and evaluated by a custom compute [kernel](#tag/kernels).\n",
        "enum": [
          "10m",
          "15m",
          "daily",
          "monthly",
          "none"
        ]
      },
      "Descriptor": {
        "type": "object",
        "description": "A Descriptor defines the nature of a time series. \nA _time series_ is defined as a series of key-value tuples, where the key is a unique Unix time stamp and the value is an integer.\nAll tuples are ordered ascending based on the time stamp.\nThe nature of a Descriptor must be constant for all associated time series tuples over all time.\nTo programmatically decide, which kind of calculations are valid, a Descriptor contains various meta information to describe what and how the value must be interpreted.\nA Descriptor must be quantifiable and must have a unit.\nTypically, a Descriptor is a _physical quantity_ or a business Descriptor like a _key performance indicator_.\n",
        "required": [
          "id",
          "key",
          "value",
          "sampling",
          "xattr"
        ],
        "properties": {
          "id": {
            "$ref": "#/components/schemas/UUID",
            "description": "The ID of the Descriptor.",
            "nullable": false
          },
          "key": {
            "type": "object",
            "properties": {
              "type": {
                "type": "string",
                "enum": [
                  "timestamp"
                ],
                "nullable": false,
                "description": "The engine has been optimized to process equidistant timestamps.\nHowever, arbitrary 64 bit keys can be processed but will degrade performance and efficiency seriously. \nThe worst case scenario is to store random numbers.\nAll build-in functions expect a timestamp anyway.\n"
              },
              "unit": {
                "type": "string",
                "enum": [
                  "seconds"
                ],
                "description": "The unit of the key must be in _seconds_. \nAlthough, the storage engine supports just a 64 bit signed integer, all build-in time-based pipeline functions\nexpect a second precision.\n"
              }
            }
          },
          "value": {
            "type": "object",
            "properties": {
              "unit": {
                "type": "string",
                "description": "The unit of the value. This is nearly arbitrary and is usually derived from physical base units.\nTypical examples are kWh, rpm or km/h.\n"
              },
              "aggregation": {
                "type": "string",
                "enum": [
                  "max",
                  "min",
                  "avg",
                  "sum",
                  "none",
                  "other"
                ],
                "description": "A data aggregation takes a bunch of values and applies a non-inverse function on it. This always means that there is a loss of information. For example, the typical 10 minute or 15 minute values have been probably aggregated from a much faster source, like a 16kHz oscilloscope sampling."
              },
              "scale": {
                "type": "integer",
                "format": "int64",
                "description": "mistral can only process integers. \nSo in case of floats or decimals this indicates the scale to divide or multiply the numbers before inserting or after querying.\nThis always has to be done explicitly to match the actual defined _unit_.\n"
              }
            }
          },
          "sampling": {
            "type": "object",
            "properties": {
              "type": {
                "type": "string",
                "enum": [
                  "periodStart",
                  "periodEnd",
                  "instant",
                  "levelBegin",
                  "levelEnd"
                ],
                "description": "The sampling describes how the combination of a key-value has been measured.\nThe following five types have been defined:\n\n## periodStart & periodEnd\nThe most natural kind is a period, where a sensor has created a bunch of samples over time.\nAt the end, there is always an aggregation involved, to return an associated single value.\n_periodStart_ defines, that the timestamp represents the beginning of the measurement.\n_periodEnd_ defines, that the timestamp represents the end of the measurement.\n\n\n```\n        0s                              600s          \n     \n            aggregation period (e.g. 600s)            \n              value (e.g. sums up to 42)              \n                      \n                                                     \n                       <-->                          \n                                                     \n                                                     \n                                  \n   start-aggregated                                 \n                                  \n                                                      \n                                                      \n                                   \n                                     end-aggregated  \n                                   \n                                                       \n                           \n               e.g. start of day in                   \n                 Australia/Eucla                      \n                                                      \n                           \n```\n\n## instant\nAlthough technically there is always a sampling period involved to capture a measurement, an instant pretends \nthat there is no relevant aggregation to represent.\nMathematically, this means, that the value has been captured within an infinite small amount of time in \nexact that moment.\n\n## levelBegin & levelEnd\nLevel defines a state of the value, which describes a system until the next time indexed value.\n_levelBegin_ defines that the value is valid since (inclusive) the timestamp until the next timestamp is found.\n_levelEnd_ defines that the value is valid until (inclusive) the timestamp for any time until the first preceeding\ntimestamp, the value is valid.\n\n```\n                                             \n                                             \n                                             \n                                             \n        \n                   \n                                           \n                                           \n                                           \n     \n    level begin            level end     \n     \n                                             \n                                             \n                                             \n```\n"
              },
              "period": {
                "nullable": false,
                "$ref": "#/components/schemas/Period"
              }
            }
          },
          "sources": {
            "deprecated": true,
            "description": "Contains arbitrary key-object mappings to attach unspecified meta data to match these against\nthird party data sources. Actually _xattr_ is the same.\n",
            "type": "object",
            "additionalProperties": {
              "type": "object",
              "properties": {
                "id": {
                  "description": "the unique id of the source",
                  "type": "string"
                },
                "value": {
                  "type": "object"
                }
              }
            }
          },
          "xattr": {
            "type": "object",
            "nullable": false,
            "description": "Arbitrary optional map of any kind of attributes.\nThis data holder can be used, to attach external or internal meta data to improve data interoperability\nor synchronizations.\nMistral never evaluates this and custom kernels should not do that either.\n"
          }
        }
      },
      "BucketType": {
        "type": "string",
        "description": "A BucketType describes the type of generator or holder of time series data.",
        "enum": [
          "wind",
          "photo",
          "solar",
          "geo",
          "bio",
          "client",
          "account",
          "other"
        ]
      },
      "Timezone": {
        "type": "string",
        "format": "timezone",
        "description": "An IANA time zone identifier like Europe/Berlin.",
        "example": "Europe/Berlin"
      },
      "Translations": {
        "type": "object",
        "description": "Translations holds text for arbitrary nested static field values. Root keys are in the RFC 5646 format.\nStructure (fields and nesting) must match the translatable values of the original resource.\n",
        "example": {
          "en": {
            "device": "my name",
            "address": {
              "tags": [
                "red",
                "Large"
              ]
            }
          },
          "de_AT": {
            "device": "Gert",
            "address": {
              "tags": [
                "Rot",
                "Gro"
              ]
            }
          },
          "fr": {
            "device": "appareil",
            "address": {
              "tags": [
                "rouge",
                "grande"
              ]
            }
          }
        }
      },
      "Bucket": {
        "type": "object",
        "description": "Bucket describes a namespace for stored metric time series data. \nA bucket usually represents a physical device like a wind turbine which \nhas an immutable physical location. Other meanings may include customer accounts \nfor financial data.\n",
        "required": [
          "id",
          "type",
          "name",
          "timezone"
        ],
        "properties": {
          "id": {
            "$ref": "#/components/schemas/UUID",
            "nullable": false
          },
          "name": {
            "type": "string",
            "description": "the default name to display.",
            "nullable": false
          },
          "description": {
            "type": "string",
            "description": "the default description about this bucket.",
            "nullable": false
          },
          "type": {
            "nullable": false,
            "$ref": "#/components/schemas/BucketType"
          },
          "timezone": {
            "nullable": false,
            "$ref": "#/components/schemas/Timezone"
          },
          "sources": {
            "nullable": false,
            "deprecated": true,
            "type": "object",
            "properties": {
              "type": {
                "type": "string",
                "description": "Type is an arbitrary id or name of to categorize the source."
              },
              "fields": {
                "type": "object",
                "description": "arbitrary map of key/values."
              }
            }
          },
          "xattr": {
            "nullable": false,
            "type": "object",
            "description": "arbitrary optional map of any kind of attributes."
          },
          "translations": {
            "$ref": "#/components/schemas/Translations"
          }
        }
      },
      "Range": {
        "description": "Range is a string representation of a range. ( or ] can be used to indicate exclusive and inclusive intervals.\n( or ) means exclusive and [ or ] means inclusive.\n\nFormat specification:\n\n  ```< [|( > < min >, < max > < ]|) > @ < IANA time zone name >```\n",
        "type": "string",
        "example": "[2038-01-19 03:14:07,2038-01-19 03:14:07]@Europe/Berlin"
      },
      "PointStream": {
        "type": "object",
        "description": "A PointStream is like a JSON array of points, but actually it is not JSON.\nInstead it uses a newline delimited json object encoding, as described by https://github.com/ndjson/ndjson-spec.\nServer and client implementations are encouraged to use a chunked encoding to avoid full buffering, so\nexpect that your implementation should parse and process until _EOF_ (end of file).\n\n```json\n{\"x\": 1653988963, \"y\": 42}\\n\n{\"x\": 1653988964, \"y\": 43}\\n\n{\"x\": 1653988965, \"y\": 44}\\n\n```\n\n_Tip: Avoid consuming point streams and instead use the [kernels API](#tag/kernels) for processing, which are\nmultiple orders of magnitudes faster. Remember, that even a laptop processor can reach a memory bandwidth of 800GB/s but a 10GBit\nLAN can only provide 1,2GB/s plus cycles and bandwidth for serialization and deserialization._\n",
        "example": "{\"x\": 1653988963, \"y\": 42}\n\n{\"x\": 1653988964, \"y\": 43}\n\n{\"x\": 1653988965, \"y\": 44}\n",
        "properties": {
          "x": {
            "type": "integer",
            "format": "int64",
            "description": "X is the unique unix timestamp, usually in seconds since Epoch.\nIt does intentionally not carry information about timezones or UTF offsets.\nFor proper calculation and transformation (like grouping by day), use the [kernels API](#tag/kernels).\nIf otherwise required, you can get the buckets time zone from the [bucket](#tag/buckets/operation/GetBucket).\n",
            "example": 1653988963
          },
          "y": {
            "type": "integer",
            "format": "int64",
            "description": "Y is the pre-scaled value for the timestamp, usually a metric value. \nConsult the metric endpoints, to learn more about the meaning of this value, e.g. what it means (like kWh)\nor how it must be post-multiplied to be displayed properly.\n",
            "example": 42
          }
        }
      },
      "BucketGroupType": {
        "type": "string",
        "description": "A BucketGroupType describes the type of group of buckets.",
        "enum": [
          "other"
        ]
      },
      "BucketGroup": {
        "type": "object",
        "description": "A BucketGroup is a collection of buckets with an arbitrary meaning.",
        "required": [
          "id",
          "name",
          "description",
          "type",
          "buckets"
        ],
        "properties": {
          "id": {
            "$ref": "#/components/schemas/UUID",
            "description": "The unique ID of the bucket group.",
            "nullable": false
          },
          "name": {
            "type": "string",
            "description": "the default name to display.",
            "nullable": false
          },
          "description": {
            "type": "string",
            "description": "the default description about this bucket.",
            "nullable": false
          },
          "type": {
            "nullable": false,
            "$ref": "#/components/schemas/BucketGroupType"
          },
          "buckets": {
            "type": "array",
            "nullable": false,
            "items": {
              "$ref": "#/components/schemas/UUID"
            }
          },
          "translations": {
            "$ref": "#/components/schemas/Translations"
          }
        }
      },
      "MiEl": {
        "type": "string",
        "example": "package main \n\nimport (\n    \"context\"\n    miel \"github.com/worldiety/mistral/lib/go/dsl/v1\"\n)\n\n// Request is an arbitrary defined input parameter type parsed from an application/json body.\n// See also lines 41 and 42 for deserialization.\ntype Request struct {\n    Portfolio miel.UUIDs         `json:\"device-ids\"`\n    Metric    miel.UUID          `json:\"metric-id\"`\n    Type      miel.AggregateFunc `json:\"type\"`\n    Range     miel.Range\n    TZ        miel.TZ\n}\n\n// Response is an arbitrary defined output parameter type serialized as an application/json body.\ntype Response struct {\n    Portfolio     miel.FPoints\n    MyDevices     miel.FGroup\n    MyDeviceNames []string\n}\n\n// Declare is a function which serves two purposes:\n//  1. declare which types are input and output parameters. This is best-practice to generate automatic documentation.\n//  2. return examples for each, also just for automatic documentation.\n// See also line 58.\nfunc Declare() (interface{}, interface{}) {\n    return Request{\n        Portfolio: miel.UUIDs{miel.UUID{}},\n        Type:      miel.AvgY,\n    }, Response{}\n}\n\n// Eval provides the actual calculation kernel and operation. It extracts concrete instances from the given context.\n// The implementation must be thread-safe and must not share any state between executions.\n// It is undefined, whether Eval is executed serializable, concurrently and/or on multiple independent Mistral cluster\n// instances at the same time.\n// See also line 59.\nfunc Eval(ctx context.Context) {\n    var request Request         // declare a variable using our custom request type\n    miel.Request(ctx, &request) // parse our custom request type\n\n    loc := request.TZ.MustParse()\n    miel.Query(ctx).\n        FindInRange(request.Portfolio, request.Metric, request.Range).\n        ForEach(func(pts miel.Points) miel.Points {\n            return pts.GroupByDay(miel.NoDrift, miel.AlignGroupStart, loc).Reduce(miel.AvgY)\n        })\n\n    miel.Response(ctx, Response{})\n}\n\n// main provides the default launching point.\nfunc main() {\n    miel.Configure().\n        Parameter(Declare).\n        Start(Eval) // eventually execute the Eval function\n}\n"
      },
      "Kernel": {
        "type": "object",
        "description": "ProcInfo contains the full set of readable meta data for a proc.",
        "required": [
          "id",
          "name",
          "description",
          "tags",
          "src"
        ],
        "properties": {
          "id": {
            "$ref": "#/components/schemas/UUID",
            "description": "The unique ID of the bucket group.",
            "nullable": false
          },
          "src": {
            "$ref": "#/components/schemas/MiEl"
          },
          "tags": {
            "type": "array",
            "description": "An arbitrary set of strings used as tags, e.g. indicating specific topics or templates.",
            "example": [
              "apexcharts",
              "daily"
            ],
            "items": {
              "type": "string"
            },
            "nullable": false
          },
          "name": {
            "type": "string",
            "description": "A short but arbitrary debug name in the default language for an end-user. Use the translations field for language specific values.",
            "example": "daily avg",
            "nullable": false
          },
          "description": {
            "type": "string",
            "description": "A longer and more descriptive text in the default language for an end-user about what the expression is about. Use the translations field for language specific values.",
            "example": "a simple daily average calculation.",
            "nullable": false
          },
          "translations": {
            "$ref": "#/components/schemas/Translations"
          }
        }
      },
      "ViewportWidth": {
        "type": "integer",
        "description": "A hint from the client for the current view port width in css pixel.",
        "example": 320,
        "default": 512
      },
      "KernelParam": {
        "type": "object",
        "description": "In general the KernelParam is an arbitrary json object. \nHowever, it must match properly to exact that struct, which is expected and parsed by a specific compute kernel.\nSee also the parameters endpoint to learn more about the request and response type specification of a specific kernel.\n",
        "example": {
          "bucketID": "550e8400-e29b-11d4-a716-446655440000",
          "metricID": "550e8400-e29b-11d4-a716-446655440000"
        }
      },
      "ParamInfo": {
        "type": "object",
        "description": "ParamInfo describes the input and output specification of a MiEL compute kernel. However, this is just a hint from sane programs.",
        "required": [
          "example"
        ],
        "properties": {
          "example": {
            "type": "object",
            "required": [
              "request",
              "response"
            ],
            "properties": {
              "request": {
                "type": "object",
                "description": "An arbitrary response example.",
                "example": {
                  "bucketID": "550e8400-e29b-11d4-a716-446655440000",
                  "metricID": "550e8400-e29b-11d4-a716-446655440000"
                }
              },
              "response": {
                "type": "object",
                "description": "An arbitrary response example.",
                "example": [
                  {
                    "x": 1,
                    "y": 2
                  },
                  {
                    "x": 3,
                    "y": 4
                  }
                ]
              }
            }
          }
        }
      },
      "BulkSeriesMergeMapping": {
        "type": "object",
        "required": [
          "dst",
          "src"
        ],
        "properties": {
          "dst": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/UUID"
            },
            "description": "Represents UUIDs of time series which are the destination to merge the source values into.",
            "nullable": false
          },
          "src": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/UUID"
            },
            "description": "Represents UUIDs of time series which are the source to load from.",
            "nullable": false
          }
        }
      },
      "BulkBucketRename": {
        "type": "object",
        "required": [
          "old",
          "new"
        ],
        "properties": {
          "old": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/UUID"
            },
            "description": "Represents UUIDs of old bucket ids.",
            "nullable": false
          },
          "new": {
            "type": "array",
            "items": {
              "$ref": "#/components/schemas/UUID"
            },
            "description": "Represents UUIDs of new bucket ids.",
            "nullable": false
          }
        }
      }
    },
    "responses": {
      "C500": {
        "description": "Internal Server Error is usually returned, if something went wrong at the server side. \nIf this problem persists, you should contact the support or your administrator and inspect the log files, to get more insight.\nTo help inspection, see the instance field of the ProblemDetails.\nSee also https://datatracker.ietf.org/doc/html/rfc7807.\n",
        "content": {
          "application/problem+json": {
            "schema": {
              "$ref": "#/components/schemas/ProblemDetails"
            }
          }
        }
      },
      "C400": {
        "description": "Indicates a bad request, which is caused by performing an invalid request, like missing or wrong formatted parameter.\nInspect the returned ProblemDetails carefully to get some insight. \nSee also https://datatracker.ietf.org/doc/html/rfc7807.\n",
        "content": {
          "application/problem+json": {
            "schema": {
              "$ref": "#/components/schemas/ProblemDetails"
            }
          }
        }
      },
      "C403": {
        "description": "Invalid bearer token. Consult your administrator or service provider to get the latest configured API token.\nSee also https://datatracker.ietf.org/doc/html/rfc7807.\n",
        "content": {
          "application/problem+json": {
            "schema": {
              "$ref": "#/components/schemas/ProblemDetails"
            }
          }
        }
      },
      "C404": {
        "description": "The requested resource has not been found.\nSee also https://datatracker.ietf.org/doc/html/rfc7807.\n",
        "content": {
          "application/problem+json": {
            "schema": {
              "$ref": "#/components/schemas/ProblemDetails"
            }
          }
        }
      },
      "C201": {
        "description": "Indicates that the request has succeeded and has created the resource.\n"
      },
      "C202": {
        "description": "Indicates that the request has been accepted for processing, but the processing has not been completed. Indeed, processing may not have started yet.\n"
      },
      "C204": {
        "description": "Indicates that the request has succeeded and the requested operation has been applied immediately and was successful. \n"
      },
      "KernelResult": {
        "description": "Returned, if the request has been parsed and executed successfully. Indeed, the kernel has returned a reasonable response, which is most likely a JSON object.\nSee also the parameters endpoint to learn more about the request and response type specification of a specific kernel.\n",
        "content": {
          "application/json": {
            "schema": {
              "type": "object",
              "example": [
                {
                  "x": 1,
                  "y": 2
                },
                {
                  "x": 3,
                  "y": 4
                }
              ]
            }
          }
        }
      }
    }
  }
}